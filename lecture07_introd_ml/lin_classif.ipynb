{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c423dbf",
   "metadata": {},
   "source": [
    "# Linear Classifiers\n",
    "\n",
    "## Fitting a line to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b980ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31343a",
   "metadata": {},
   "source": [
    "Supose we are measuring how a mouse weight (X) can be used to predict the mouse size(Y). The values are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28813263",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([0.60316714, 5.13981077, 0.57754654, 3.35880456, 5.28171939,\n",
    "        9.41578636, 2.43742198, 5.99075038, 2.49605785, 6.83781763,\n",
    "        0.16296473, 9.29969598])\n",
    "Y = np.array([15.15613261, 23.89223832, 15.72151754, 16.35859565, 22.06175073,\n",
    "        27.36346235, 20.4802553 , 24.54353801, 21.22924112, 21.77229456,\n",
    "        14.94636364, 30.70479942])\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X, Y, color='blue', label='Random Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6590cff2",
   "metadata": {},
   "source": [
    "We train to find a good line that fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(line, color, ax, values):\n",
    "    min_value = np.min(values)\n",
    "    max_value = np.max(values)\n",
    "    # Generate x-values\n",
    "    x = np.linspace(min_value, max_value, 100)  # range of x-values\n",
    "    y = line(x)\n",
    "    ax.plot(x, y, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e9f52c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X, Y, color='blue', label='Random Points')\n",
    "line = np.poly1d([1.2, 16])\n",
    "draw_line(line, 'red', ax, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c1d31b",
   "metadata": {},
   "source": [
    "Now it comes the question, is it a good one? Is it the best posible line to fit the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b3d56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X, Y, color='blue', label='Random Points')\n",
    "draw_line(np.poly1d([1.2, 16]), 'red', ax, X)\n",
    "draw_line(np.poly1d([1.4, 14]), 'gray', ax, X)\n",
    "draw_line(np.poly1d([0.8, 16]), 'cyan', ax, X)\n",
    "draw_line(np.poly1d([0, 22]), 'green', ax, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2181b559",
   "metadata": {},
   "source": [
    "We can measure now how well the line fits the data by seen how close is it to the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30643b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "distances = [abs(line(x) - y) for x, y in zip(X, Y)]\n",
    "print(distances)\n",
    "print(sum(distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef00d1ac",
   "metadata": {},
   "source": [
    "Since we want to penalize larger divergences, we square the terms (additionally, the *abs* have some nasty mathematical properties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e188b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [(line(x) - y)**2 for x, y in zip(X, Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec187be",
   "metadata": {},
   "source": [
    "if we add all the distances, the result is named **sum of squared residuals (SSR)**, because the **residuals** are the differences between the real and estimated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3df9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c91f92",
   "metadata": {},
   "source": [
    "Now lets create a function for performing the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967bb173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_sq_res(line, X, Y):\n",
    "    return sum((line(x) - y)**2 for x, y in zip(X, Y))\n",
    "\n",
    "sum_sq_res(line, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d2c7d",
   "metadata": {},
   "source": [
    "Lets evaluate the functions we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f7046",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_lines = [\n",
    "    (np.poly1d([1.2, 16]), 'red'), \n",
    "    (np.poly1d([1.4, 14]), 'gray'),\n",
    "    (np.poly1d([0.8, 16]), 'cyan'),\n",
    "    (np.poly1d([0, 22]), 'green'),\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X, Y, color='blue', label='Random Points')\n",
    "for l, color in all_lines:\n",
    "    draw_line(l, color, ax, X)\n",
    "    print(color, sum_sq_res(l, X, Y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481bc56",
   "metadata": {},
   "source": [
    "As you can see, the better the line fit the data, the smaller value it have on the SSR. \n",
    "\n",
    "Lets try to find the line with the minimal value. This method is called **Least Squares**. We need to find two values:\n",
    "- The curve slope, that controls the angle with respect to the horizontal axis\n",
    "- The curve intercept, that controls the point where the curve cuts the vertical axis.\n",
    "\n",
    "Consider the horizontal line with the average _y_ value. This is not a good one, but since it is based on data, will be our starting point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e7588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b = np.average(Y)\n",
    "print(b)\n",
    "line = np.poly1d([0, b])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X, Y, color='blue', label='Random Points')\n",
    "draw_line(line, 'red', ax, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c245403",
   "metadata": {},
   "source": [
    "Now we explore the influence of the slope in the SSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec90131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for slope in np.arange(-4, 4, 0.1):\n",
    "    line = np.poly1d([slope, b])\n",
    "    ssr = sum_sq_res(line, X, Y)\n",
    "    print(slope, ssr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855dd0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# using a comprehension\n",
    "points = [(slope, sum_sq_res(np.poly1d([slope, b]), X, Y)) for slope in np.arange(-5, 5, 0.1)]\n",
    "ssrs = np.array(points)\n",
    "plt.plot(ssrs[:, 0],ssrs[:, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98229e3",
   "metadata": {},
   "source": [
    "The ssr function has a derivative that is easy to calculate, allowing us to directly locate the minimum by finding the point where the derivative equals zero.\n",
    "\n",
    "For this example, since we have a computer, we can directly get the slope with lowest ssr value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_row_index = np.argmin(ssrs[:, 1])\n",
    "min_slope = ssrs[min_row_index, 0]\n",
    "min_slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bcd93a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b = np.average(Y)\n",
    "print(b)\n",
    "line = np.poly1d([min_slope, b])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X, Y, color='blue', label='Random Points')\n",
    "draw_line(line, 'red', ax, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c859cb",
   "metadata": {},
   "source": [
    "We can do a similar operation in both parameters simultaneously to find the best line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc392af",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [(slope, c, sum_sq_res(np.poly1d([slope, c]), X, Y)) \n",
    "          for slope in np.arange(-5, 5, 0.1) \n",
    "          for c in np.arange(10, 20, 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1afdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array(points)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0e0018",
   "metadata": {},
   "source": [
    "Lets plot the SSR values with respect to slope and c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1966e6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the points in 3D\n",
    "ax.scatter(points[:, 0], points[:, 1], points[:, 2], c=points[:, 2])\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('c')\n",
    "ax.set_zlabel('SSV')\n",
    "ax.set_title('3D Scatter Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf85226",
   "metadata": {},
   "source": [
    "And get the lowest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b6ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_row_index = np.argmin(points[:,2])\n",
    "min_slope, min_c, ssr = points[min_row_index]\n",
    "min_slope, min_c, ssr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58748877",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "line = np.poly1d([min_slope, min_c])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X, Y, color='blue', label='Random Points')\n",
    "draw_line(line, 'red', ax, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4527909",
   "metadata": {},
   "source": [
    "Summarizing:\n",
    "- To fit the model to the available data we need to minimize the sum of squared residual from the model and the data\n",
    "- To do this, if we know a good candidate parameter interval, we can explore it. \n",
    "    - We can also analytically calculate the partial derivatives and find the point where it is zero\n",
    "- The point where the SSR is minimal can be used in the model. derivative is zero is the one that minimizes the SSR, so it must be used for the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168423c3",
   "metadata": {},
   "source": [
    "## Evaluating the quality of fitting, the R^2\n",
    "\n",
    "Lets start by taking only the mouse size, and plotting the line of its average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d71173",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_size = np.mean(Y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(np.full(len(Y), 4), Y, color='blue', label='Random Points')\n",
    "draw_line(np.poly1d([0, mean_size]), 'red', ax, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e91481",
   "metadata": {},
   "source": [
    "Now, calculate the SSR around the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c45e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssr_mean = sum((mean_size - y)**2 for y in Y)\n",
    "ssr_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ca0583",
   "metadata": {},
   "source": [
    "The variance around the mean is calculated dividing ssr_mean by the number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd9f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Var_mean = ssr_mean / len(Y)\n",
    "Var_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f3e12b",
   "metadata": {},
   "source": [
    "Now calculate the SSR for our best estimation of the line that relates mouse weight and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ca42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = np.poly1d([min_slope, min_c])\n",
    "ssr_fit = sum((line(x) - y)**2 for x, y in zip(X, Y))\n",
    "ssr_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae2c97",
   "metadata": {},
   "source": [
    "And its variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62bc26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Var_fit = ssr_fit / len(Y)\n",
    "Var_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8674100",
   "metadata": {},
   "source": [
    "We can see that a part of the variation of the the Y variable can be \"explained\" by taking the X variable into account.\n",
    "- The variation in the mouse size can be explained partially by taking the mouse weight into cosideration.\n",
    "\n",
    "R^2 tells us how much of the variation in the mouse size can be explained by taking the mouse weight into acount. The equation is the following:\n",
    "\n",
    "R^2 = (Var_mean - Var_fit) / Var_mean.\n",
    "\n",
    "Since the number of elements divide both variances, we can simplify by:\n",
    "\n",
    "R^2 = (ssr_mean - ssr_fit) / ssr_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75faac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_2 = (ssr_mean - ssr_fit) / ssr_mean\n",
    "R_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc7522",
   "metadata": {},
   "source": [
    "This means that almost 85% of the variation in mouse size can be explained by taking weight into account using the calculated model. This is quite good indeed!!\n",
    "\n",
    "Lets see another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ef3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2 = line(X)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X, Y2, color='blue', label='Random Points')\n",
    "draw_line(line, 'red', ax, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb14e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssr_mean = sum((mean_size - y)**2 for y in Y2)\n",
    "ssr_fit = sum((line(x) - y)**2 for x, y in zip(X, Y2))\n",
    "R_2 = (ssr_mean - ssr_fit) / ssr_mean\n",
    "R_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043c10b",
   "metadata": {},
   "source": [
    "We can see that 100% of the variance in the Y variable can be explained by the X variable using the line equation. Perfect match!\n",
    "\n",
    "Lets see a final example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb4435",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X3 = np.random.randint(1, 10, size=15)\n",
    "Y3 = np.random.randint(1, 10, size=15)\n",
    "line3 = np.poly1d([0, np.average(Y3)])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X3, Y3, color='blue', label='Random Points')\n",
    "draw_line(line3, 'red', ax, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac9ba1",
   "metadata": {},
   "source": [
    "Lets calculate R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba69aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_size = np.average(Y3)\n",
    "ssr_mean = sum((mean_size - y)**2 for y in Y3)\n",
    "ssr_fit = sum((line3(x) - y)**2 for x, y in zip(X3, Y3))\n",
    "R_2 = (ssr_mean - ssr_fit) / ssr_mean\n",
    "R_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6147b",
   "metadata": {},
   "source": [
    "In this case, since R^2 is zero, knowing the mouse weight do not helps us to explain the mouse size, and the model we built is useless.\n",
    "\n",
    "**Note**. We apply this analysis to lines, but it can be perfomed to any other model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a72579",
   "metadata": {},
   "source": [
    "## Linear classifiers in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/heart.csv')\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
    "data[\"ChestPainType\"] = label_encoder.fit_transform(data[\"ChestPainType\"])\n",
    "data[\"RestingECG\"] = label_encoder.fit_transform(data[\"RestingECG\"])\n",
    "data[\"ExerciseAngina\"] = label_encoder.fit_transform(data[\"ExerciseAngina\"])\n",
    "data[\"ST_Slope\"] = label_encoder.fit_transform(data[\"ST_Slope\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56aaef",
   "metadata": {},
   "source": [
    "### Simple linear regression models\n",
    "Lets try to predict the person Age, based on the other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19a96a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"Coefficients:\", regressor.coef_)\n",
    "print(\"Intercept:\", regressor.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9013f815",
   "metadata": {},
   "source": [
    "And evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf400f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Calculate predicted values\n",
    "y_pred = regressor.predict(X)\n",
    "\n",
    "# Calculate sum of squared errors (SSE)\n",
    "sse = mean_squared_error(y, y_pred) * len(y)\n",
    "\n",
    "# Calculate total sum of squares (SST)\n",
    "sst = np.sum((y - np.mean(y))**2)\n",
    "\n",
    "# Calculate R-squared value\n",
    "r_squared = 1 - (sse / sst)\n",
    "\n",
    "# Calculate p-values using statsmodels\n",
    "X_t = sm.add_constant(X)  # Add constant column for intercept\n",
    "model = sm.OLS(y, X_t)\n",
    "results = model.fit()\n",
    "p_values = results.pvalues\n",
    "\n",
    "# Print results\n",
    "print(\"Sum of Squared Errors (SSE):\", sse)\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"P-values:\")\n",
    "for k, p in p_values.items():\n",
    "    print(k, str(round(p, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debbc184",
   "metadata": {},
   "source": [
    "### Logistic classifier\n",
    "A logistic classifier, also known as logistic regression, is a popular classification algorithm used in machine learning. \n",
    "- It is primarily used for binary classification tasks, where the goal is to predict the probability of an input belonging to a particular class.\n",
    "- The logistic classifier utilizes the logistic function, also called the sigmoid function, to model the relationship between the input features and the target class probabilities. \n",
    "- The sigmoid function maps any real-valued input to a value between 0 and 1, representing the probability of the input belonging to the positive class.\n",
    "\n",
    "In this example, we have the probability of mouse to be obese based on its weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376f1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weights = [1,2, 2.5, 3, 4, 5, 4, 5, 6, 8, 9, 10]\n",
    "obese_prob = [ 0,0,0,0,0, 0,0,1,1,1,1, 1]\n",
    "\n",
    "X = np.array([[x] for x in weights])\n",
    "y = np.array(obese_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a8e5e",
   "metadata": {},
   "source": [
    "Now, lets train a Logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270dead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Initialize and train the logistic regression classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary\n",
    "x_min, x_max = X.min() - 1, X.max() + 1\n",
    "xx = np.linspace(x_min, x_max, 100).reshape(-1, 1)\n",
    "y_pred = clf.predict_proba(xx)[:, 1]\n",
    "\n",
    "plt.plot(xx, y_pred)\n",
    "plt.scatter(X, y, c='red')\n",
    "plt.ylabel('Obese probability')\n",
    "plt.xlabel('Mouse weight')\n",
    "plt.title('Logistic Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbeab52",
   "metadata": {},
   "source": [
    "- The learned model reveals that the probability of obesity tends to approach zero when the weight is closer to 2 or less. \n",
    "- Conversely, the probability of obesity tends to approach one when the weight is closer to 8. \n",
    "- It is important to note that the resulting probabilities range from 0 to 1, even though the training data only contains zeros and ones.\n",
    "\n",
    "Now, lets try this model with real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288ed42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data.copy()\n",
    "data_new['isOld'] = data.Age > 54\n",
    "data_new = data_new.drop(['Age'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2da51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_new.drop(['isOld'], axis=1)\n",
    "y = data_new['isOld']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c071d",
   "metadata": {},
   "source": [
    "Now we evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a8a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e1139",
   "metadata": {},
   "source": [
    "This is, for the first 10 objects, the probability of classification in class 0 and 1, or 'young' and 'old'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551ff58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.hstack([y_pred, y_test.to_frame()])[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79094e",
   "metadata": {},
   "source": [
    "Lets evaluate with accuracy, which assign the class to the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04febfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate predicted values\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febcd921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_lectures]",
   "language": "python",
   "name": "conda-env-python_lectures-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
