{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5c0e61",
   "metadata": {
    "id": "cIU7W8Wmebey"
   },
   "source": [
    "# Introduction to OpenCV\n",
    "\n",
    "## Getting started\n",
    "\n",
    "First we need to import the relevant libraries: OpenCV itself, Numpy, and a couple of others. Common and Video are simple data handling and opening routines that you can find in the OpenCV Python Samples directory or from the github repo linked above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5cf0cd",
   "metadata": {
    "id": "TXKxw8iJebez"
   },
   "outputs": [],
   "source": [
    "# These imports let you use opencv\n",
    "import cv2 #opencv itself\n",
    "import numpy as np # matrix manipulations\n",
    "\n",
    "#the following are to do with this interactive notebook code\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt # this lets you draw inline pictures in the notebooks\n",
    "import pylab # this allows you to control figure size\n",
    "pylab.rcParams['figure.figsize'] = (5, 5) # this controls figure size in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d11346c",
   "metadata": {
    "id": "htK6mm-Gebe2"
   },
   "source": [
    "\n",
    "\n",
    "Now we can open an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05c2ba",
   "metadata": {
    "id": "Ah762ATHebe3"
   },
   "outputs": [],
   "source": [
    "input_image=cv2.imread('noidea.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c150b",
   "metadata": {
    "id": "ut1_Lwdgebe5"
   },
   "source": [
    "We can find out various things about that image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c240df1",
   "metadata": {
    "id": "awdTYn4Gebe6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(input_image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf502692",
   "metadata": {
    "id": "af7iQyhqebe8"
   },
   "outputs": [],
   "source": [
    "print(input_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0229ed2",
   "metadata": {
    "id": "UhxrodZrebe_"
   },
   "outputs": [],
   "source": [
    "print(input_image.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9bef96",
   "metadata": {
    "id": "stSDqhuBebfA"
   },
   "source": [
    "**gotcha** that last one (datatype) is one of the tricky things about working in Python. As it's not strongly typed, Python will allow you to have arrays of different types but the same size, and some functions will return arrays of types that you probably don't want. Being able to check and inspect the datatype like this is very useful and is one of the things I often find myself doing in debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed38452",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"lolo\", input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015f2c99",
   "metadata": {
    "id": "woP9RhyCebfB"
   },
   "outputs": [],
   "source": [
    "plt.imshow(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9db107",
   "metadata": {
    "collapsed": true,
    "id": "6VFxWhvUebfD"
   },
   "source": [
    "What this illustrates is something key about OpenCV: it doesn't store images in RGB format, but in BGR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6882b",
   "metadata": {
    "id": "zgEQX0isebfD"
   },
   "outputs": [],
   "source": [
    "# split channels\n",
    "b,g,r=cv2.split(input_image)\n",
    "# show one of the channels (this is red - see that the sky is kind of dark. try changing it to b)\n",
    "plt.imshow(b, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80655d86",
   "metadata": {
    "id": "XqE1jCKaebfG"
   },
   "source": [
    "## converting between colour spaces, merging and splitting channels\n",
    "\n",
    "We can convert between various colourspaces in OpenCV easily. We've seen how to split, above. We can also merge channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b217098b",
   "metadata": {
    "id": "Ev_3hJKLebfH"
   },
   "outputs": [],
   "source": [
    "merged=cv2.merge([r,g,b])\n",
    "# merge takes an array of single channel matrices\n",
    "plt.imshow(merged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e63f1",
   "metadata": {
    "id": "cJ-UCAynebfJ"
   },
   "source": [
    "OpenCV also has a function specifically for dealing with image colorspaces, so rather than split and merge channels by hand you can use this instead. It is usually marginally faster...\n",
    "\n",
    "There are something like 250 color related flags in OpenCV for conversion and display. The ones you are most likely to use are COLOR_BGR2RGB for RGB conversion, COLOR_BGR2GRAY for conversion to greyscale, and COLOR_BGR2HSV for conversion to Hue,Saturation,Value colour space. [http://docs.opencv.org/trunk/de/d25/imgproc_color_conversions.html] has more information on how these colour conversions are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa775b",
   "metadata": {
    "id": "egPmVUvYebfK"
   },
   "outputs": [],
   "source": [
    "COLORflags = [flag for flag in dir(cv2) if flag.startswith('COLOR') ]\n",
    "print(len(COLORflags))\n",
    "\n",
    "# If you want to see them all, rather than just a count uncomment the following line\n",
    "# print(COLORflags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d269d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images(images, titles=None, figsize=(10, 6), cmap=\"gray\"):\n",
    "    if titles is None:\n",
    "        titles = \" \" * len(images)\n",
    "    fig, axes = plt.subplots(len(images), 1, figsize=figsize)\n",
    "\n",
    "    for i, (image, title) in enumerate(zip(images, titles)):\n",
    "        axes[i].imshow(image, cmap)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff990d",
   "metadata": {
    "id": "INRZEZdvebfM"
   },
   "outputs": [],
   "source": [
    "hsv_image=cv2.cvtColor(input_image, cv2.COLOR_BGR2YCR_CB)\n",
    "x, y, z =cv2.split(hsv_image)\n",
    "plot_images([x, y, z], [\"Y\", \"Cr\", \"Cb\"], (10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb987f97",
   "metadata": {
    "id": "lfo1Lue9ebfN"
   },
   "source": [
    "## Getting image data and setting image data\n",
    "\n",
    "Images in python OpenCV are numpy arrays. Numpy arrays are optimised for fast array operations and so there are usually fast methods for doing array calculations which don't actually involve writing all the detail yourself. So it's usually bad practice to access individual pixels, but you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5859e59",
   "metadata": {
    "id": "32AQVQ0uebfO"
   },
   "outputs": [],
   "source": [
    "pixel = input_image[100,100]\n",
    "print(pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973775c2",
   "metadata": {
    "id": "OE1vlYo2ebfQ"
   },
   "outputs": [],
   "source": [
    "input_image[100,100] = [0,0,0]\n",
    "pixelnew = input_image[100,100]\n",
    "print(pixelnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca7bac4",
   "metadata": {
    "id": "KsFd9SBzebfS"
   },
   "source": [
    "## Getting and setting regions of an image\n",
    "\n",
    "In the same way as we can get or set individual pixels, we can get or set regions of an image. This is a particularly useful way to get a region of interest to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d2fde",
   "metadata": {
    "id": "D0rwsf8sebfS"
   },
   "outputs": [],
   "source": [
    "dogface = input_image[60:250, 70:350]\n",
    "plt.imshow(dogface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292a97c",
   "metadata": {
    "id": "i1xm1L5MebfU"
   },
   "outputs": [],
   "source": [
    "fresh_image=cv2.imread('noidea.jpg') # it's either start with a fresh read of the image,\n",
    "                                  # or end up with dogfaces on dogfaces on dogfaces\n",
    "                                   # as you re-run parts of the notebook but not others...\n",
    "\n",
    "fresh_image[200:200+dogface.shape[0], 200:200+dogface.shape[1]]=dogface\n",
    "print(dogface.shape[0])\n",
    "print(dogface.shape[1])\n",
    "plt.imshow(fresh_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7025e073",
   "metadata": {
    "collapsed": true,
    "id": "bCVZFlDhebfW"
   },
   "source": [
    "## Matrix slicing\n",
    "In OpenCV python style, as I have mentioned, images are numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0fd99f",
   "metadata": {
    "id": "9LLhUPE7ebfX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freshim2 = cv2.imread(\"noidea.jpg\")\n",
    "crop = freshim2[100:400, 130:300]\n",
    "plt.imshow(crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff59b3",
   "metadata": {
    "id": "3pURgAtbebfZ"
   },
   "source": [
    "The key thing to note here is that the slicing works like\n",
    "```\n",
    "[top_y:bottom_y, left_x:right_x]\n",
    "```\n",
    "This can also be thought of as\n",
    "```\n",
    "[y:y+height, x:x+width]\n",
    "```\n",
    "\n",
    "You can also use slicing to separate out channels.  In this case you want\n",
    "```\n",
    "[y:y+height, x:x+width, channel]\n",
    "```\n",
    "where channel represents the colour you're interested in - this could be 0 = blue, 1 = green or 2=red if you're dealing with a default OpenCV image, but if you've got an image that has been converted it could be something else. Here's an example that converts to HSV then selects the S (Saturation) channel of the same crop above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f09aecc",
   "metadata": {
    "id": "9cSa7WDHebfZ"
   },
   "outputs": [],
   "source": [
    "hsvim=cv2.cvtColor(freshim2,cv2.COLOR_BGR2HSV)\n",
    "bcrop =hsvim[100:400, 100:300, 1]\n",
    "plt.imshow(bcrop, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff574f",
   "metadata": {},
   "source": [
    "## Image stats and image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d05f0",
   "metadata": {
    "id": "b8PY3kZ63fWO"
   },
   "source": [
    "### Basic manipulations\n",
    "\n",
    "Rotate, flip..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e21496",
   "metadata": {
    "id": "4LHzdNvt3fWP"
   },
   "outputs": [],
   "source": [
    "flipped_code_0=cv2.flip(input_image,0) # vertical flip\n",
    "plt.imshow(flipped_code_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab3e4ea",
   "metadata": {
    "id": "9SOq_oD-3fWR"
   },
   "outputs": [],
   "source": [
    "flipped_code_1=cv2.flip(input_image,1) # horizontal flip\n",
    "plt.imshow(flipped_code_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe5314",
   "metadata": {
    "id": "7zapvC1p3fWU"
   },
   "outputs": [],
   "source": [
    "transposed=cv2.transpose(input_image)\n",
    "plt.imshow(transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021783ae",
   "metadata": {
    "id": "n8yTtUGq3fWX"
   },
   "source": [
    "### Minimum, maximum\n",
    "\n",
    "To find the min or max of a matrix, you can use minMaxLoc. This takes a single channel image (it doesn't make much sense to take the max of a 3 channel image). So in the next code snippet you see a for loop, using python style image slicing, to look at each channel of the input image separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60b2bf",
   "metadata": {
    "id": "S7nLP0QL3fWY"
   },
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    min_value, max_value, min_location, max_location=cv2.minMaxLoc(input_image[:,:,i])\n",
    "    print(\"min {} is at {}, and max {} is at {}\".format(min_value, min_location, max_value, max_location))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc46725",
   "metadata": {
    "id": "qF_AsupK3fWa"
   },
   "source": [
    "### Arithmetic operations on images\n",
    "\n",
    "OpenCV has a lot of functions for doing mathematics on images. Some of these have \"analogous\" numpy alternatives, but it is nearly always better to use the OpenCV version. The reason for this that OpenCV is designed to work on images and so handles overflow better (OpenCV add, for example, truncates to 255 if the datatype is image-like and 8 bit; Numpy's alternative wraps around).\n",
    "\n",
    "Useful arithmetic operations include add and addWeighted, which combine two images that are the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99b3a0",
   "metadata": {
    "id": "Gg5M59Lt3fWa"
   },
   "outputs": [],
   "source": [
    "#First create an image the same size as our input\n",
    "blank_image = np.zeros((input_image.shape), np.uint8)\n",
    "\n",
    "blank_image[100:200,100:200,1]=100; #give it a green square\n",
    "\n",
    "new_image=cv2.add(blank_image,input_image) # add the two images together\n",
    "\n",
    "plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f131c",
   "metadata": {
    "id": "ke-afLFp3fWd"
   },
   "source": [
    "### Noise reduction\n",
    "Noise reduction usually involves blurring/smoothing an image using a Gaussian kernel.\n",
    "The width of the kernel determines the amount of smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e865aa",
   "metadata": {
    "id": "EyxmwP0E3fWd"
   },
   "outputs": [],
   "source": [
    "d=3\n",
    "img_blur3 = cv2.GaussianBlur(input_image, (2*d+1, 2*d+1), -1)[d:-d,d:-d]\n",
    "plot_images([input_image, img_blur3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f461f",
   "metadata": {
    "id": "GjGY7Dl33fWg"
   },
   "outputs": [],
   "source": [
    "d=5\n",
    "img_blur5 = cv2.GaussianBlur(input_image, (2*d+1, 2*d+1), -1)[d:-d,d:-d]\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_blur5, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f50b88",
   "metadata": {
    "id": "AaJ7zd1w3fWi"
   },
   "outputs": [],
   "source": [
    "d=15\n",
    "img_blur15 = cv2.GaussianBlur(input_image, (2*d+1, 2*d+1), -1)[d:-d,d:-d]\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_blur15, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb9673",
   "metadata": {
    "id": "Hp50ZT_A3fWk"
   },
   "source": [
    "### Edges\n",
    "\n",
    "For a lot of what we think of as \"modern\" computer vision techniques, edge detection functions as a building block. Much edge detection actually works by **convolution**, and indeed **convolutional neural networks** are absolutely the flavour of the month in some parts of computer vision. Sobel's edge detector was one of the first truly successful edge detection (enhancement) technique and that involves convolution at its core. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5906297f",
   "metadata": {
    "id": "d7ceQSv13fWk"
   },
   "outputs": [],
   "source": [
    "sobelimage=cv2.cvtColor(input_image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "sobelx = cv2.Sobel(sobelimage,cv2.CV_64F,1,0,ksize=9)\n",
    "sobely = cv2.Sobel(sobelimage,cv2.CV_64F,0,1,ksize=9)\n",
    "plt.imshow(sobelx,cmap = 'gray')\n",
    "# Sobel works in x and in y, change sobelx to sobely in the olt line above to see the difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4b8eb",
   "metadata": {
    "id": "RWxFMGQe3fWm"
   },
   "source": [
    "Canny edge detection is another winnning technique - it takes two thresholds.\n",
    "The first one determines how likely Canny is to find an edge, and the second determines how likely it is to follow that edge once it's found. Investigate the effect of these thresholds by altering the values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc8ac3",
   "metadata": {
    "id": "1MJQRgXL3fWn"
   },
   "outputs": [],
   "source": [
    "th1=30\n",
    "th2=60 # Canny recommends threshold 2 is 3 times threshold 1 - you could try experimenting with this...\n",
    "d=3 # gaussian blur\n",
    "\n",
    "edgeresult=input_image.copy()\n",
    "edgeresult = cv2.GaussianBlur(edgeresult, (2*d+1, 2*d+1), -1)[d:-d,d:-d]\n",
    "\n",
    "gray = cv2.cvtColor(edgeresult, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "edge = cv2.Canny(gray, th1, th2)\n",
    "\n",
    "edgeresult[edge != 0] = (0, 255, 0) # this takes pixels in edgeresult where edge non-zero colours them bright green\n",
    "\n",
    "plt.imshow(cv2.cvtColor(edgeresult, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1f57e",
   "metadata": {
    "id": "X7HD5UmG5NY-"
   },
   "source": [
    "## Features in computer vision\n",
    "\n",
    "Features are image locations that are \"easy\" to find in the future.  Indeed, one of the early feature detection techniques Lucas-Kanade, sometimes called Kanade-Lucas-Tomasi or KLT features come from a seminal paper called \"Good features to track\".\n",
    "\n",
    "Edges find brightness discontinuities in an image, features find distinctive regions. There are a bunch of different feature detectors and these all have some characteristics in common: they should be quick to find, and things that are close in image-space are close in feature-space (that is, the feature representation of an object looks like the feature representation of objects that look like that object)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d7f25",
   "metadata": {
    "id": "yW11cMb15NZB"
   },
   "source": [
    "### Corner detectors\n",
    "If you think of edges as being lines, then corners are an obvious choice for features as they represent the intersection of two lines. One of the earlier corner detectors was introduced by Harris, and it is still a very effective corner detector that gets used quite a lot: it's reliable and it's fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886855f",
   "metadata": {
    "id": "JdU2qm635NZC"
   },
   "outputs": [],
   "source": [
    "harris_test=input_image.copy()\n",
    "#greyscale it\n",
    "gray = cv2.cvtColor(harris_test,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = np.float32(gray)\n",
    "blocksize=4 #\n",
    "kernel_size=3 # sobel kernel: must be odd and fairly small\n",
    "\n",
    "# run the harris corner detector\n",
    "dst = cv2.cornerHarris(gray,blocksize,kernel_size,0.05) # parameters are blocksize, Sobel parameter and Harris threshold\n",
    "\n",
    "#result is dilated for marking the corners, this is visualisation related and just makes them bigger\n",
    "dst = cv2.dilate(dst,None)\n",
    "#we then plot these on the input image for visualisation purposes, using bright red\n",
    "harris_test[dst>0.01*dst.max()]=[0,0,255]\n",
    "plt.imshow(cv2.cvtColor(harris_test, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a057e25",
   "metadata": {
    "collapsed": true,
    "id": "0_6wW-nY5NZH"
   },
   "source": [
    "## Moving towards feature space\n",
    "When we consider modern feature detectors there are a few things we need to mention. What makes a good feature includes the following:\n",
    "\n",
    "* Repeatability (got to be able to find it again)\n",
    "* Distinctiveness/informativeness (features representing different things need to be different)\n",
    "* Locality (they need to be local to the image feature and not, like, the whole image)\n",
    "* Quantity (you need to be able to find enough of them for them to be properly useful)\n",
    "* Accuracy (they need to accurately locate the image feature)\n",
    "* Efficiency (they've got to be computable in reasonable time)\n",
    "\n",
    "This comes from a good survey which you can find here (and which I'd thoroughly recommend reading if you're doing feature detection work) [here](https://www.slideshare.net/AhmedOne1/survey-1-project-overview)\n",
    "\n",
    "**Note:** some of the very famous feature detectors (SIFT/SURF and so on) are around, but aren't in OpenCV by default due to patent issues. You can build them for OpenCV if you want - or you can find other implementations (David Lowe's SIFT implementation works just fine). Just google for instructions.  For the purposes of this tutorial (and to save time) we're only going to look at those which are actually in OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaad6a7",
   "metadata": {
    "id": "DXu8K_FJ5NZI"
   },
   "outputs": [],
   "source": [
    "orbimg=input_image.copy()\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "# find the keypoints with ORB\n",
    "kp = orb.detect(orbimg,None)\n",
    "# compute the descriptors with ORB\n",
    "kp, des = orb.compute(orbimg, kp)\n",
    "# draw keypoints\n",
    "cv2.drawKeypoints(orbimg,kp,orbimg)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.imshow(cv2.cvtColor(orbimg, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63244cf7",
   "metadata": {
    "collapsed": true,
    "id": "tynAouFx5NZK"
   },
   "source": [
    "## Matching features\n",
    "Finding features is one thing but actually we want to use them for matching.\n",
    "First let's get something where we know there's going to be a match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f467b1",
   "metadata": {
    "id": "xyjEKvnK5NZK"
   },
   "outputs": [],
   "source": [
    "img2match=np.zeros(input_image.shape,np.uint8)\n",
    "dogface=input_image[60:250, 70:350] # copy out a bit\n",
    "img2match[60:250,70:350]=[0,0,0] # blank that region\n",
    "dogface=cv2.flip(dogface,0) #flip the copy\n",
    "img2match[200:200+dogface.shape[0], 200:200+dogface.shape[1]]=dogface # paste it back somewhere else\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img2match, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d8250",
   "metadata": {
    "id": "-pzjjdcD5NZN"
   },
   "source": [
    "## Matching keypoints\n",
    "\n",
    "The feature matching function (in this case Orb) detects and then computes keypoint descriptors. These are a higher dimensional representation of the image region immediately around a point of interest (sometimes literally called \"interest points\").\n",
    "\n",
    "These higher-dimensional representations can then be matched; the strength you gain from matching these descriptors rather than image regions directly is that they have a certain invariance to transformations (like rotation, or scaling). OpenCV providers matcher routines to do this, in which you can specify the distance measure to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af54e3",
   "metadata": {
    "id": "a8pk6NeP5NZN"
   },
   "outputs": [],
   "source": [
    "\n",
    "kp2 = orb.detect(img2match,None)\n",
    "# compute the descriptors with ORB\n",
    "kp2, des2 = orb.compute(img2match, kp2)\n",
    "# create BFMatcher object: this is a Brute Force matching object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "# Match descriptors.\n",
    "matches = bf.match(des,des2)\n",
    "\n",
    "# Sort them by distance between matches in feature space - so the best matches are first.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "# Draw first 50 matches.\n",
    "oimg = cv2.drawMatches(orbimg,kp,img2match,kp2,matches[:50], orbimg)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.imshow(cv2.cvtColor(oimg, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd26e32",
   "metadata": {
    "id": "E2qdpBTV5NZP"
   },
   "source": [
    "As you can see there are some false matches, but it's fairly clear that most of the matched keypoints found are actual matches between image regions on the dogface.\n",
    "\n",
    "To be more precise about our matching we could choose to enforce **homography** constraints, which looks for features than sit on the same plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6ffde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_lectures]",
   "language": "python",
   "name": "conda-env-python_lectures-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
