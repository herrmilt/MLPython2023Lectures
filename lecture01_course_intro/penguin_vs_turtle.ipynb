{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd211c95",
   "metadata": {},
   "source": [
    "# Penguin vs Turtle image classification\n",
    "\n",
    "We have the task of automatically determine if an image contains a Turtle or a Penguin:\n",
    "- We use the machine learning way of doing it, collect images from both classes\n",
    "- We use Tensorflow to build the classification model. TensorFlow is an open-source machine learning framework developed by Google. It is widely used for tasks such as deep learning, neural networks, and numerical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df45cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "#from IPython.display import YouTubeVideo\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c238dd",
   "metadata": {},
   "source": [
    "It is really important to configure GPUs if you have. If not, training networks can take a really long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03235253",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    print('GPU', tf.test.gpu_device_name(), 'configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b6421",
   "metadata": {},
   "source": [
    "Load the data. I split the images in two folders, one for training the model and other one for evaluating the models with images that it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dba9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_path = \"data/penguin_turtles/train\"\n",
    "train_annot_file_name = \"data/penguin_turtles/train_annotations\"\n",
    "test_base_path = \"data/penguin_turtles/valid\"\n",
    "test_annot_file_name = \"data/penguin_turtles/valid_annotations\"\n",
    "train_file_names = [os.path.join(train_base_path, f) for f in os.listdir(train_base_path)]\n",
    "valid_file_names = [os.path.join(train_base_path, f) for f in os.listdir(test_base_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac8bea",
   "metadata": {},
   "source": [
    "Lets display some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333328b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Assuming you have a list of image file names called 'image_files'\n",
    "image_files = train_file_names[:16]\n",
    "\n",
    "# Calculate the number of rows and columns for the grid\n",
    "n_rows = 4\n",
    "n_cols = 4\n",
    "\n",
    "# Create a new figure with the desired grid size\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5, 5))\n",
    "\n",
    "# Iterate over the image files and display them in the grid\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(image_files):\n",
    "        img = mpimg.imread(image_files[i])\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "# Adjust the spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21868e97",
   "metadata": {},
   "source": [
    "Seed everything to reproduce results for future use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    # Seed value for TensorFlow\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # Seed value for NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Seed value for Python's random library\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Force TensorFlow to use single thread\n",
    "    # Multiple threads are a potential source of non-reproducible results.\n",
    "    session_conf = tf.compat.v1.ConfigProto(\n",
    "        intra_op_parallelism_threads=1,\n",
    "        inter_op_parallelism_threads=1\n",
    "    )\n",
    "\n",
    "    # Make sure that TensorFlow uses a deterministic operation wherever possible\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a932767",
   "metadata": {},
   "source": [
    "Read the annotation file.\n",
    "\n",
    "- Note: Every dataset available in internet might have a different way to store and annotate the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00255483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_anno=pd.read_json(train_annot_file_name)\n",
    "train_anno.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa87dd6a",
   "metadata": {},
   "source": [
    "Lets check the number of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see how many classes are present in the category id\n",
    "classes=list(train_anno['category_id'].unique())\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdcf7b2",
   "metadata": {},
   "source": [
    "Transform datasets to my selected format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataframe(df):\n",
    "    df=df.drop(['id', 'bbox', 'area', 'segmentation', 'iscrowd'], axis =1)\n",
    "    # lets replace the numeric class designation with the test name of the class\n",
    "    df['category_id']=df['category_id'].replace({1:'Penguin',2:'Turtle'})\n",
    "    df.columns=['filepaths', 'labels']\n",
    "    df['filepaths'] = df['filepaths'].apply(\n",
    "        lambda x: os.path.join(train_base_path, f\"image_id_{str(x).zfill(3)}.jpg\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = transform_dataframe(train_anno)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(train_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd7953",
   "metadata": {},
   "source": [
    "Do the same with testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.read_json(test_annot_file_name)\n",
    "test_df = transform_dataframe(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c63066",
   "metadata": {},
   "source": [
    "Lets check the balance in the number of elements per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance=list(train_df['labels'].value_counts())\n",
    "print (balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac657bed",
   "metadata": {},
   "source": [
    "The train set is balance and has an adequate number of images for classification\n",
    "\n",
    "Lets create generators for the three dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=(224,224) # image size\n",
    "bs=40 # batch size\n",
    "t_gen=ImageDataGenerator(horizontal_flip=True) # generator for training for data augmentation\n",
    "gen=ImageDataGenerator() # generator for validation and test \n",
    "train_gen=t_gen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   color_mode='rgb',batch_size=bs, shuffle=True, seed=123,\n",
    "                                   class_mode= 'categorical')\n",
    "valid_gen=gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   color_mode='rgb',batch_size=bs, shuffle=False, seed=123,\n",
    "                                   class_mode= 'categorical')\n",
    "test_gen=gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   color_mode='rgb',batch_size=bs, shuffle=False, seed=123,\n",
    "                                   class_mode= 'categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fcd3ac",
   "metadata": {},
   "source": [
    "Lets create now the neural network. In this example we use transfer learning using as base the EfficientNetB0 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count=2 # number of classes\n",
    "lr= .001 # learning rate\n",
    "img_shape=(224,224,3) # shape of color images\n",
    "base_model=tf.keras.applications.EfficientNetV2B0(include_top=False, weights=\"imagenet\",\n",
    "                                                   input_shape=img_shape, pooling='max')        \n",
    "base_model.trainable=True\n",
    "x=base_model.output\n",
    "x=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
    "x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
    "                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
    "x = Dropout(rate=.4, seed=123)(x)       \n",
    "output=Dense(class_count, activation='softmax')(x)\n",
    "model=Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1c8eb",
   "metadata": {},
   "source": [
    "Compile the model, defining the learner, the loss and quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb862cf",
   "metadata": {},
   "source": [
    "Lets define 2 callbacks , reduce learning rate on plateau and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e3587",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlronp=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.4,  patience=2,\n",
    "                                            verbose=1)\n",
    "estop=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4,restore_best_weights=True)\n",
    "callbacks=[rlronp, estop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d0d8d",
   "metadata": {},
   "source": [
    "Now lets train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "history=model.fit(x=train_gen,   epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n",
    "                   validation_steps=None,  shuffle=True,  initial_epoch=0) # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4474aa8",
   "metadata": {},
   "source": [
    "Lets define a function to plot the training data. (copied from Internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c4c81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tr_plot(tr_data):\n",
    "    start_epoch=0\n",
    "    #Plot the training and validation data\n",
    "    tacc=tr_data.history['accuracy']\n",
    "    tloss=tr_data.history['loss']\n",
    "    vacc=tr_data.history['val_accuracy']\n",
    "    vloss=tr_data.history['val_loss']      \n",
    "    Epoch_count=len(tacc)+ start_epoch\n",
    "    Epochs=[]\n",
    "    for i in range (start_epoch ,Epoch_count):\n",
    "        Epochs.append(i+1)   \n",
    "    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n",
    "    val_lowest=vloss[index_loss]\n",
    "    index_acc=np.argmax(vacc)\n",
    "    acc_highest=vacc[index_acc]    \n",
    "    plt.style.use('fivethirtyeight')\n",
    "    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
    "    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)     \n",
    "    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(25,10))\n",
    "    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
    "    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
    "    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
    "    axes[0].scatter(Epochs, tloss, s=100, c='red')    \n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs', fontsize=18)\n",
    "    axes[0].set_ylabel('Loss', fontsize=18)\n",
    "    axes[0].legend()\n",
    "    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
    "    axes[1].scatter(Epochs, tacc, s=100, c='red')\n",
    "    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
    "    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epochs', fontsize=18)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=18)\n",
    "    axes[1].legend()    \n",
    "    plt.tight_layout    \n",
    "    plt.show()\n",
    "    return \n",
    "\n",
    "tr_plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba6fd2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We ended up with a validation accuracy over 98% so the model is performing correctly.\n",
    "\n",
    "Lets make predictions on the test set and produce a confusion matrix and a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65225c4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predictor(model,test_gen):\n",
    "    classes=list(test_gen.class_indices.keys())\n",
    "    class_count=len(classes)\n",
    "    preds=model.predict(test_gen, verbose=1)\n",
    "    errors=0\n",
    "    test_count =len(preds)\n",
    "    misclassified_classes=[]\n",
    "    misclassified_files=[]\n",
    "    misclassified_as = []\n",
    "    pred_indices=[]\n",
    "    for i, p in enumerate (preds):\n",
    "        pred_index=np.argmax(p)\n",
    "        pred_indices.append(pred_index)\n",
    "        true_index= test_gen.labels[i]    \n",
    "        if  pred_index != true_index:        \n",
    "            errors +=1        \n",
    "            misclassified_classes.append(classes[true_index])\n",
    "            misclassified_as.append(classes[pred_index])\n",
    "            file=test_gen.filenames[i]\n",
    "            split=file.split('/')\n",
    "            L=len(split)           \n",
    "            f=split[L-2] +' '+ split[L-1]  \n",
    "            misclassified_files.append(f)\n",
    "\n",
    "    accuracy = (test_count-errors)*100/test_count\n",
    "    ytrue=np.array(test_gen.labels)\n",
    "    ypred=np.array(pred_indices)\n",
    "    f1score=f1_score(ytrue, ypred, average='weighted')* 100\n",
    "    msg=f'There were {errors} errors in {test_count} tests for an accuracy of {accuracy:6.2f} and an F1 score of {f1score:6.2f}'\n",
    "    print (msg) \n",
    "    misclassified_classes=sorted(misclassified_classes)\n",
    "    if len(misclassified_classes) > 0:\n",
    "        misclassifications=[]\n",
    "        for klass in misclassified_classes:\n",
    "            mis_count=misclassified_classes.count(klass)\n",
    "            misclassifications.append(mis_count)\n",
    "        unique=len(np.unique(misclassified_classes)) \n",
    "        if unique==1:\n",
    "            height=int(unique)\n",
    "        else:\n",
    "            height =int(unique/2)\n",
    "        plt.figure(figsize=(10, height))\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        plt.barh(misclassified_classes, misclassifications )\n",
    "        plt.title( 'Classification Errors on Test Set by Class', fontsize=20, color='blue')\n",
    "        plt.xlabel('NUMBER OF MISCLASSIFICATIONS', fontsize=20, color='blue')\n",
    "        plt.ylabel('CLASS', fontsize=20, color='blue')\n",
    "        plt.show()\n",
    "    if class_count <=30:\n",
    "        cm = confusion_matrix(ytrue, ypred )\n",
    "        # plot the confusion matrix\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "        plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n",
    "        plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "        clr = classification_report(ytrue, ypred, target_names=classes, digits= 4) # create classification report\n",
    "        print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "    return f1score, misclassified_files\n",
    "\n",
    "f1score, misclassified_files= predictor(model,test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc70a63",
   "metadata": {},
   "source": [
    "Save the model for later reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,subject, classes, img_size, f1score, working_dir):    \n",
    "    name=f'{subject}-{str(len(classes))}-({str(img_size[0])} X {str(img_size[1])})- {f1score:5.2f}.h5'    \n",
    "    model_save_loc=os.path.join(working_dir, name)\n",
    "    try:\n",
    "        model.save(model_save_loc)        \n",
    "        msg= f'model was saved as {model_save_loc}'        \n",
    "    except:\n",
    "        msg='model can not be saved due to tensorflow 2.10.0 or higher due to bug for efficientnet models' \n",
    "    print (msg)\n",
    "    \n",
    "subject='penguins vs turtles'\n",
    "working_dir=r'data/penguin_turtles/'\n",
    "save_model(model,subject, classes, img_size, f1score, working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1778889b",
   "metadata": {},
   "source": [
    "## Later use of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9301bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('data/penguin_turtles/penguins vs turtles-2-(224 X 224)- 99.00.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {0:'Penguin',1:'Turtle'}\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_files = []\n",
    "image_labels = []\n",
    "for image_path in valid_file_names[:20]:\n",
    "    image = load_img(image_path, target_size=(224, 224))  # Adjust the target_size as per your model's input shape\n",
    "    image_files.append(image)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(image_array)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    image_labels.append(class_names[predicted_class])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c77e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows and columns for the grid\n",
    "n_rows = 4\n",
    "n_cols = 4\n",
    "\n",
    "# Create a new figure with the desired grid size\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "\n",
    "# Iterate over the image files and display them in the grid with labels\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(image_files):\n",
    "        ax.imshow(image_files[i])\n",
    "        ax.set_title(image_labels[i])\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "# Adjust the spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_lectures",
   "language": "python",
   "name": "python_lectures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
